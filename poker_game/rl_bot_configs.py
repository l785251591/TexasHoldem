#!/usr/bin/env python3
"""
å¼ºåŒ–å­¦ä¹ æœºå™¨äººé…ç½®é›†åˆ
é€šè¿‡è°ƒæ•´ä¸åŒå‚æ•°åˆ›å»ºå„ç§ç±»å‹çš„æœºå™¨äºº

è¿™ä¸ªæ–‡ä»¶å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨æŠ½è±¡åŸºç±»å’Œé…ç½®ç³»ç»Ÿæ¥åˆ›å»ºä¸åŒé£æ ¼çš„æœºå™¨äººï¼š
- é€šè¿‡è°ƒæ•´å­¦ä¹ å‚æ•°æ¥æ”¹å˜å­¦ä¹ é€Ÿåº¦å’Œæ¢ç´¢ç­–ç•¥
- é€šè¿‡è°ƒæ•´ç­–ç•¥å‚æ•°æ¥æ”¹å˜æ¸¸æˆé£æ ¼
- é€šè¿‡å¼€å…³é«˜çº§åŠŸèƒ½æ¥ä¼˜åŒ–æ€§èƒ½
"""

from .base_rl_bot import RLBotConfig

# ===== é¢„å®šä¹‰é…ç½® =====

def get_original_config() -> RLBotConfig:
    """åŸç‰ˆæœºå™¨äººé…ç½® - ä¿å®ˆå­¦ä¹ ï¼ŒåŸºç¡€åŠŸèƒ½"""
    return RLBotConfig(
        # å­¦ä¹ å‚æ•° (ä¿å®ˆè®¾ç½®)
        epsilon=0.3,
        epsilon_decay=0.9995,
        epsilon_min=0.05,
        learning_rate=0.01,
        gamma=0.9,
        
        # ç­–ç•¥å‚æ•° (ä¿å®ˆé£æ ¼)
        aggression_threshold=0.7,
        max_bet_ratio=0.4,
        min_hand_strength_call=0.3,
        min_hand_strength_raise=0.65,
        all_in_threshold=0.9,
        
        # é«˜çº§åŠŸèƒ½ (åŸºç¡€åŠŸèƒ½)
        use_double_q_learning=False,
        use_experience_replay=False,
        use_enhanced_state=False,
        use_dynamic_actions=False,
        use_conservative_mode=False,
        
        memory_size=5000,
        replay_batch_size=16,
        snapshot_interval=50,
        model_name="rl_bot"
    )

def get_improved_config() -> RLBotConfig:
    """æ”¹è¿›ç‰ˆæœºå™¨äººé…ç½® - æ¿€è¿›å­¦ä¹ ï¼Œå…¨éƒ¨åŠŸèƒ½"""
    return RLBotConfig(
        # å­¦ä¹ å‚æ•° (æ¿€è¿›è®¾ç½®)
        epsilon=0.35,
        epsilon_decay=0.9997,
        epsilon_min=0.08,
        learning_rate=0.015,
        gamma=0.95,
        
        # ç­–ç•¥å‚æ•° (æ¿€è¿›é£æ ¼)
        aggression_threshold=0.55,
        max_bet_ratio=0.6,
        min_hand_strength_call=0.2,
        min_hand_strength_raise=0.5,
        all_in_threshold=0.8,
        
        # é«˜çº§åŠŸèƒ½ (å…¨éƒ¨å¯ç”¨)
        use_double_q_learning=True,
        use_experience_replay=True,
        use_enhanced_state=True,
        use_dynamic_actions=True,
        use_conservative_mode=False,
        
        memory_size=10000,
        replay_batch_size=32,
        snapshot_interval=50,
        model_name="improved_rl_bot"
    )

def get_conservative_config() -> RLBotConfig:
    """ä¿å®ˆç‰ˆæœºå™¨äººé…ç½® - ç¨³å¥å­¦ä¹ ï¼Œä¿å®ˆç­–ç•¥"""
    return RLBotConfig(
        # å­¦ä¹ å‚æ•° (å¹³è¡¡è®¾ç½®)
        epsilon=0.15,
        epsilon_decay=0.9998,
        epsilon_min=0.03,
        learning_rate=0.008,
        gamma=0.92,
        
        # ç­–ç•¥å‚æ•° (ä¿å®ˆé£æ ¼)
        aggression_threshold=0.6,
        max_bet_ratio=0.3,
        min_hand_strength_call=0.2,
        min_hand_strength_raise=0.65,
        all_in_threshold=0.85,
        
        # é«˜çº§åŠŸèƒ½ (é€‰æ‹©æ€§å¯ç”¨)
        use_double_q_learning=True,
        use_experience_replay=True,
        use_enhanced_state=True,
        use_dynamic_actions=True,
        use_conservative_mode=True,
        
        memory_size=8000,
        replay_batch_size=24,
        snapshot_interval=50,
        model_name="conservative_rl_bot"
    )

# ===== ç‰¹æ®Šé£æ ¼é…ç½® =====

def get_aggressive_config() -> RLBotConfig:
    """è¶…æ¿€è¿›æœºå™¨äººé…ç½® - æåº¦æ¿€è¿›ï¼Œé«˜é£é™©é«˜å›æŠ¥"""
    return RLBotConfig(
        epsilon=0.4,           # é«˜æ¢ç´¢ç‡
        epsilon_decay=0.9995,
        epsilon_min=0.1,       # ä¿æŒè¾ƒé«˜æ¢ç´¢
        learning_rate=0.02,    # å¿«é€Ÿå­¦ä¹ 
        gamma=0.95,
        
        aggression_threshold=0.3,  # å¾ˆä½çš„æ¿€è¿›é˜ˆå€¼
        max_bet_ratio=0.8,         # å…è®¸å¤§é¢ä¸‹æ³¨
        min_hand_strength_call=0.1,
        min_hand_strength_raise=0.3,
        all_in_threshold=0.6,      # å®¹æ˜“å…¨æŠ¼
        
        use_double_q_learning=True,
        use_experience_replay=True,
        use_enhanced_state=True,
        use_dynamic_actions=True,
        
        memory_size=15000,
        replay_batch_size=64,
        model_name="aggressive_rl_bot"
    )

def get_tight_config() -> RLBotConfig:
    """è¶…ä¿å®ˆæœºå™¨äººé…ç½® - æåº¦ä¿å®ˆï¼Œåªç©å¼ºç‰Œ"""
    return RLBotConfig(
        epsilon=0.1,           # ä½æ¢ç´¢ç‡
        epsilon_decay=0.999,
        epsilon_min=0.01,      # æä½æ¢ç´¢
        learning_rate=0.005,   # ç¼“æ…¢å­¦ä¹ 
        gamma=0.9,
        
        aggression_threshold=0.8,  # å¾ˆé«˜çš„æ¿€è¿›é˜ˆå€¼
        max_bet_ratio=0.2,         # åªå…è®¸å°é¢ä¸‹æ³¨
        min_hand_strength_call=0.4,
        min_hand_strength_raise=0.8,
        all_in_threshold=0.95,     # å‡ ä¹ä»ä¸å…¨æŠ¼
        
        use_double_q_learning=True,
        use_experience_replay=True,
        use_enhanced_state=True,
        use_dynamic_actions=True,
        use_conservative_mode=True,
        
        memory_size=5000,
        replay_batch_size=16,
        model_name="tight_rl_bot"
    )

def get_bluff_config() -> RLBotConfig:
    """è¯ˆå”¬å‹æœºå™¨äººé…ç½® - å–„äºè¯ˆå”¬å’Œå¿ƒç†æˆ˜"""
    return RLBotConfig(
        epsilon=0.25,
        epsilon_decay=0.9996,
        epsilon_min=0.05,
        learning_rate=0.012,
        gamma=0.93,
        
        aggression_threshold=0.4,  # ä¸­ç­‰æ¿€è¿›é˜ˆå€¼
        max_bet_ratio=0.7,         # å…è®¸è¾ƒå¤§ä¸‹æ³¨
        min_hand_strength_call=0.15,
        min_hand_strength_raise=0.25,  # ä½è¦æ±‚åŠ æ³¨(è¯ˆå”¬)
        all_in_threshold=0.7,
        
        use_double_q_learning=True,
        use_experience_replay=True,
        use_enhanced_state=True,
        use_dynamic_actions=True,
        
        memory_size=12000,
        replay_batch_size=40,
        model_name="bluff_rl_bot"
    )

def get_adaptive_config() -> RLBotConfig:
    """é€‚åº”å‹æœºå™¨äººé…ç½® - å–„äºé€‚åº”å¯¹æ‰‹"""
    return RLBotConfig(
        epsilon=0.3,
        epsilon_decay=0.9999,      # å¾ˆæ…¢çš„è¡°å‡ï¼Œä¿æŒé€‚åº”æ€§
        epsilon_min=0.08,
        learning_rate=0.018,       # è¾ƒå¿«å­¦ä¹ ä»¥é€‚åº”
        gamma=0.96,                # é‡è§†é•¿æœŸå›æŠ¥
        
        aggression_threshold=0.6,
        max_bet_ratio=0.5,
        min_hand_strength_call=0.25,
        min_hand_strength_raise=0.55,
        all_in_threshold=0.85,
        
        use_double_q_learning=True,
        use_experience_replay=True,
        use_enhanced_state=True,
        use_dynamic_actions=True,
        
        memory_size=20000,         # å¤§å®¹é‡è®°å¿†
        replay_batch_size=48,
        model_name="adaptive_rl_bot"
    )

# ===== å®éªŒæ€§é…ç½® =====

def get_experimental_config() -> RLBotConfig:
    """å®éªŒæ€§é…ç½® - ç”¨äºæµ‹è¯•æ–°åŠŸèƒ½"""
    return RLBotConfig(
        epsilon=0.5,               # é«˜æ¢ç´¢ç‡
        epsilon_decay=0.999,
        epsilon_min=0.15,
        learning_rate=0.025,       # é«˜å­¦ä¹ ç‡
        gamma=0.98,                # é«˜æŠ˜æ‰£å› å­
        
        aggression_threshold=0.5,
        max_bet_ratio=0.6,
        min_hand_strength_call=0.2,
        min_hand_strength_raise=0.4,
        all_in_threshold=0.75,
        
        use_double_q_learning=True,
        use_experience_replay=True,
        use_enhanced_state=True,
        use_dynamic_actions=True,
        
        memory_size=25000,
        replay_batch_size=64,
        snapshot_interval=25,      # æ›´é¢‘ç¹çš„å¿«ç…§
        model_name="experimental_rl_bot"
    )

# ===== é…ç½®å­—å…¸ =====

PREDEFINED_CONFIGS = {
    'original': get_original_config,
    'improved': get_improved_config,
    'conservative': get_conservative_config,
    'aggressive': get_aggressive_config,
    'tight': get_tight_config,
    'bluff': get_bluff_config,
    'adaptive': get_adaptive_config,
    'experimental': get_experimental_config,
}

def get_config_by_name(name: str) -> RLBotConfig:
    """æ ¹æ®åç§°è·å–é¢„å®šä¹‰é…ç½®"""
    if name in PREDEFINED_CONFIGS:
        return PREDEFINED_CONFIGS[name]()
    else:
        raise ValueError(f"æœªçŸ¥é…ç½®åç§°: {name}. å¯ç”¨é…ç½®: {list(PREDEFINED_CONFIGS.keys())}")

def create_custom_config(**kwargs) -> RLBotConfig:
    """åˆ›å»ºè‡ªå®šä¹‰é…ç½®
    
    å‚æ•°:
        **kwargs: ä»»ä½•RLBotConfigæ”¯æŒçš„å‚æ•°
    
    ç¤ºä¾‹:
        config = create_custom_config(
            epsilon=0.2,
            learning_rate=0.01,
            aggression_threshold=0.5,
            model_name="my_custom_bot"
        )
    """
    return RLBotConfig(**kwargs)

# ===== é…ç½®æ¯”è¾ƒå·¥å…· =====

def compare_configs(*configs: RLBotConfig) -> dict:
    """æ¯”è¾ƒå¤šä¸ªé…ç½®çš„å·®å¼‚"""
    if not configs:
        return {}
    
    comparison = {}
    
    # è·å–æ‰€æœ‰å‚æ•°å
    all_params = set()
    for config in configs:
        all_params.update(config.__dict__.keys())
    
    # æ¯”è¾ƒæ¯ä¸ªå‚æ•°
    for param in sorted(all_params):
        values = []
        for i, config in enumerate(configs):
            value = getattr(config, param, "N/A")
            values.append(f"Config{i+1}: {value}")
        comparison[param] = values
    
    return comparison

def print_config_summary(config: RLBotConfig):
    """æ‰“å°é…ç½®æ‘˜è¦"""
    print(f"\nğŸ“‹ {config.model_name} é…ç½®æ‘˜è¦:")
    print("â”€" * 50)
    
    print("ğŸ¯ å­¦ä¹ å‚æ•°:")
    print(f"   æ¢ç´¢ç‡: {config.epsilon} â†’ {config.epsilon_min} (è¡°å‡: {config.epsilon_decay})")
    print(f"   å­¦ä¹ ç‡: {config.learning_rate}")
    print(f"   æŠ˜æ‰£å› å­: {config.gamma}")
    
    print("\nğŸ® ç­–ç•¥å‚æ•°:")
    print(f"   æ¿€è¿›é˜ˆå€¼: {config.aggression_threshold}")
    print(f"   æœ€å¤§ä¸‹æ³¨æ¯”ä¾‹: {config.max_bet_ratio*100:.0f}%")
    print(f"   è·Ÿæ³¨é—¨æ§›: {config.min_hand_strength_call}")
    print(f"   åŠ æ³¨é—¨æ§›: {config.min_hand_strength_raise}")
    print(f"   å…¨æŠ¼é—¨æ§›: {config.all_in_threshold}")
    
    print("\nâš™ï¸ é«˜çº§åŠŸèƒ½:")
    features = []
    if config.use_double_q_learning:
        features.append("åŒQå­¦ä¹ ")
    if config.use_experience_replay:
        features.append("ç»éªŒå›æ”¾")
    if config.use_enhanced_state:
        features.append("å¢å¼ºçŠ¶æ€")
    if config.use_dynamic_actions:
        features.append("åŠ¨æ€åŠ¨ä½œ")
    if config.use_conservative_mode:
        features.append("ä¿å®ˆæ¨¡å¼")
    
    print(f"   å¯ç”¨: {', '.join(features) if features else 'åŸºç¡€åŠŸèƒ½'}")
    print(f"   è®°å¿†å®¹é‡: {config.memory_size}")
    print(f"   å›æ”¾æ‰¹æ¬¡: {config.replay_batch_size}")

# ===== ä½¿ç”¨ç¤ºä¾‹ =====

if __name__ == "__main__":
    # å±•ç¤ºä¸åŒé…ç½®
    print("ğŸ¤– å¼ºåŒ–å­¦ä¹ æœºå™¨äººé…ç½®ç³»ç»Ÿ")
    print("=" * 60)
    
    # å±•ç¤ºé¢„å®šä¹‰é…ç½®
    for name in ['original', 'improved', 'conservative', 'aggressive']:
        config = get_config_by_name(name)
        print_config_summary(config)
    
    # å±•ç¤ºè‡ªå®šä¹‰é…ç½®
    print("\nğŸ”§ è‡ªå®šä¹‰é…ç½®ç¤ºä¾‹:")
    custom_config = create_custom_config(
        epsilon=0.25,
        learning_rate=0.015,
        aggression_threshold=0.45,
        max_bet_ratio=0.6,
        model_name="custom_balanced_bot"
    )
    print_config_summary(custom_config) 