#!/usr/bin/env python3
"""
å¼ºåŒ–å­¦ä¹ æœºå™¨äººæŠ½è±¡åŸºç±»
æä¾›å¯é…ç½®çš„å‚æ•°ç³»ç»Ÿï¼Œæ”¯æŒä¸åŒç­–ç•¥çš„å®ç°
"""

from abc import ABC, abstractmethod
from .player import Player, PlayerAction
from typing import Dict, Any, Tuple, List
import pickle
import os
import random
from collections import defaultdict, deque
import math

class RLBotConfig:
    """å¼ºåŒ–å­¦ä¹ æœºå™¨äººé…ç½®ç±»"""
    
    def __init__(self, 
                 # å­¦ä¹ å‚æ•°
                 epsilon: float = 0.3,
                 epsilon_decay: float = 0.9995,
                 epsilon_min: float = 0.05,
                 learning_rate: float = 0.01,
                 gamma: float = 0.9,
                 
                 # ç­–ç•¥å‚æ•°
                 aggression_threshold: float = 0.7,
                 max_bet_ratio: float = 0.5,
                 min_hand_strength_call: float = 0.25,
                 min_hand_strength_raise: float = 0.6,
                 all_in_threshold: float = 0.9,
                 
                 # é«˜çº§åŠŸèƒ½å¼€å…³
                 use_double_q_learning: bool = True,
                 use_experience_replay: bool = True,
                 use_enhanced_state: bool = True,
                 use_dynamic_actions: bool = True,
                 use_conservative_mode: bool = False,
                 
                 # ç»éªŒå›æ”¾å‚æ•°
                 memory_size: int = 10000,
                 replay_batch_size: int = 32,
                 
                 # å…¶ä»–å‚æ•°
                 snapshot_interval: int = 50,
                 model_name: str = "base_rl_bot"
                 ):
        
        # å­¦ä¹ å‚æ•°
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min
        self.learning_rate = learning_rate
        self.gamma = gamma
        
        # ç­–ç•¥å‚æ•°
        self.aggression_threshold = aggression_threshold
        self.max_bet_ratio = max_bet_ratio
        self.min_hand_strength_call = min_hand_strength_call
        self.min_hand_strength_raise = min_hand_strength_raise
        self.all_in_threshold = all_in_threshold
        
        # é«˜çº§åŠŸèƒ½
        self.use_double_q_learning = use_double_q_learning
        self.use_experience_replay = use_experience_replay
        self.use_enhanced_state = use_enhanced_state
        self.use_dynamic_actions = use_dynamic_actions
        self.use_conservative_mode = use_conservative_mode
        
        # ç»éªŒå›æ”¾å‚æ•°
        self.memory_size = memory_size
        self.replay_batch_size = replay_batch_size
        
        # å…¶ä»–å‚æ•°
        self.snapshot_interval = snapshot_interval
        self.model_name = model_name

class BaseRLBot(Player, ABC):
    """å¼ºåŒ–å­¦ä¹ æœºå™¨äººæŠ½è±¡åŸºç±»"""
    
    def __init__(self, player_id: str, name: str, chips: int = 1000, 
                 config: RLBotConfig = None, model_path: str = None):
        super().__init__(player_id, name, chips)
        
        # ä½¿ç”¨é…ç½®
        self.config = config or self.get_default_config()
        
        # è®¾ç½®æ¨¡å‹è·¯å¾„
        if model_path:
            self.model_path = model_path
        else:
            self.model_path = f"models/{self.config.model_name}_model.pkl"
        
        # åº”ç”¨é…ç½®å‚æ•°
        self._apply_config()
        
        # åˆå§‹åŒ–å­¦ä¹ ç»„ä»¶
        self._initialize_learning_components()
        
        # åŠ è½½æ¨¡å‹
        self.load_model()
    
    @abstractmethod
    def get_default_config(self) -> RLBotConfig:
        """è·å–é»˜è®¤é…ç½®ï¼Œå­ç±»å¿…é¡»å®ç°"""
        pass
    
    def _apply_config(self):
        """åº”ç”¨é…ç½®å‚æ•°"""
        config = self.config
        
        # å­¦ä¹ å‚æ•°
        self.epsilon = config.epsilon
        self.epsilon_decay = config.epsilon_decay
        self.epsilon_min = config.epsilon_min
        self.learning_rate = config.learning_rate
        self.gamma = config.gamma
        
        # ç­–ç•¥å‚æ•°
        self.aggression_threshold = config.aggression_threshold
        self.max_bet_ratio = config.max_bet_ratio
        self.min_hand_strength_call = config.min_hand_strength_call
        self.min_hand_strength_raise = config.min_hand_strength_raise
        self.all_in_threshold = config.all_in_threshold
        
        # åŠŸèƒ½å¼€å…³
        self.use_double_q_learning = config.use_double_q_learning
        self.use_experience_replay = config.use_experience_replay
        self.use_enhanced_state = config.use_enhanced_state
        self.use_dynamic_actions = config.use_dynamic_actions
        self.conservative_mode = config.use_conservative_mode
        
        # ç»éªŒå›æ”¾å‚æ•°
        self.memory_size = config.memory_size
        self.replay_batch_size = config.replay_batch_size
        
        # å…¶ä»–å‚æ•°
        self.snapshot_interval = config.snapshot_interval
    
    def _initialize_learning_components(self):
        """åˆå§‹åŒ–å­¦ä¹ ç»„ä»¶"""
        # Qè¡¨ (æ ¹æ®é…ç½®å†³å®šæ˜¯å¦ä½¿ç”¨åŒQå­¦ä¹ )
        if self.use_double_q_learning:
            self.q_table_1 = defaultdict(lambda: defaultdict(float))
            self.q_table_2 = defaultdict(lambda: defaultdict(float))
        else:
            self.q_table = defaultdict(lambda: defaultdict(float))
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.total_reward = 0.0
        self.game_count = 0
        self.win_count = 0
        
        # çŠ¶æ€è®¿é—®è®¡æ•°
        self.state_visit_count = defaultdict(int)
        self.action_count = defaultdict(lambda: defaultdict(int))
        
        # ç»éªŒå›æ”¾ç¼“å†²åŒº
        if self.use_experience_replay:
            self.experience_memory = deque(maxlen=self.memory_size)
        
        # è®­ç»ƒè½¨è¿¹
        self.current_trajectory = []
        self.last_state_key = None
        self.last_action_key = None
        
        # å½“å‰æ‰‹ç‰Œä¿¡æ¯
        self.total_bet_in_hand = 0
        self.is_folded = False
        
        print(f"ğŸ“ åˆ›å»ºæ–°çš„{self.config.model_name}: {self.model_path}")
    
    def get_state_key(self, game_state: Dict[str, Any]) -> str:
        """è·å–çŠ¶æ€é”®å€¼"""
        if self.use_enhanced_state:
            return self._get_enhanced_state_key(game_state)
        else:
            return self._get_basic_state_key(game_state)
    
    def _get_basic_state_key(self, game_state: Dict[str, Any]) -> str:
        """è·å–åŸºç¡€çŠ¶æ€è¡¨ç¤º"""
        try:
            hand_strength = self.estimate_hand_strength(game_state)
            pot_odds = self.calculate_pot_odds(game_state)
            
            # ç¦»æ•£åŒ–
            hand_str_bucket = min(4, int(hand_strength * 5))
            pot_odds_bucket = min(3, int(pot_odds * 2)) if pot_odds < float('inf') else 3
            
            opponents_count = len([p for p in game_state.get('other_players', []) 
                                  if not p.get('is_folded', False)])
            opponents_count = max(1, min(opponents_count, 5))
            
            return f"h{hand_str_bucket}_p{pot_odds_bucket}_o{opponents_count}"
        except:
            return "default_state"
    
    def _get_enhanced_state_key(self, game_state: Dict[str, Any]) -> str:
        """è·å–å¢å¼ºçŠ¶æ€è¡¨ç¤º"""
        try:
            # åŸºç¡€ä¿¡æ¯
            hand_strength = self.estimate_hand_strength(game_state)
            pot_odds = self.calculate_pot_odds(game_state)
            
            # å¯¹æ‰‹ä¿¡æ¯
            opponents_count = len([p for p in game_state.get('other_players', []) 
                                  if not p.get('is_folded', False)])
            opponents_count = max(1, min(opponents_count, 8))
            
            # ä¸‹æ³¨ä¿¡æ¯
            call_amount = game_state.get('call_amount', 0)
            pot_size = game_state.get('pot', 0)
            
            # ä½ç½®ä¿¡æ¯
            position_type = self._get_position_type(opponents_count)
            
            # æ¸¸æˆé˜¶æ®µ
            community_cards = game_state.get('community_cards', [])
            stage = len(community_cards)  # 0=preflop, 3=flop, 4=turn, 5=river
            
            # ç¦»æ•£åŒ–å„ä¸ªç»´åº¦
            hand_bucket = min(4, int(hand_strength * 5))
            pot_odds_bucket = min(3, int(pot_odds * 2)) if pot_odds < float('inf') else 3
            
            # ä¸‹æ³¨å‹åŠ›
            bet_pressure = call_amount / max(self.chips, 1)
            pressure_bucket = min(3, int(bet_pressure * 4))
            
            # åº•æ± ç›¸å¯¹å¤§å°
            pot_ratio = pot_size / max(self.chips + pot_size, 1)
            pot_bucket = min(3, int(pot_ratio * 4))
            
            return (f"h{hand_bucket}_p{pot_odds_bucket}_o{opponents_count}_"
                   f"pos{position_type}_s{stage}_pr{pressure_bucket}_pb{pot_bucket}")
        except:
            return "enhanced_default"
    
    def _get_position_type(self, opponents_count: int) -> str:
        """è·å–ä½ç½®ç±»å‹"""
        if opponents_count <= 2:
            return "hu"  # heads-up
        elif opponents_count <= 4:
            return "sh"  # short-handed
        else:
            return "fu"  # full table
    
    def get_action(self, game_state: Dict[str, Any]) -> Tuple[PlayerAction, int]:
        """è·å–åŠ¨ä½œ"""
        try:
            state_key = self.get_state_key(game_state)
            self.state_visit_count[state_key] += 1
            
            # è·å–å¯ç”¨åŠ¨ä½œ
            if self.use_dynamic_actions:
                available_actions = self._get_enhanced_actions(game_state)
            else:
                available_actions = self._get_basic_actions(game_state)
            
            # é€‰æ‹©åŠ¨ä½œ
            if self._should_explore(state_key):
                # æ¢ç´¢ï¼šéšæœºé€‰æ‹©
                action, amount = random.choice(available_actions)
            else:
                # åˆ©ç”¨ï¼šé€‰æ‹©æœ€ä½³åŠ¨ä½œ
                action, amount = self._get_best_action(state_key, available_actions, game_state)
            
            # è®°å½•åŠ¨ä½œ
            action_key = self._get_action_key(action, amount, game_state)
            self.action_count[state_key][action_key] += 1
            
            # ä¿å­˜çŠ¶æ€å’ŒåŠ¨ä½œ
            self.last_state_key = state_key
            self.last_action_key = action_key
            
            # è®°å½•è½¨è¿¹
            self.current_trajectory.append({
                'state': state_key,
                'action': (action, amount),
                'game_state': game_state.copy()
            })
            
            return action, amount
            
        except Exception as e:
            print(f"âš ï¸  {self.name}: å†…éƒ¨è®¡ç®—å¼‚å¸¸ (å·²è‡ªåŠ¨å¤„ç†)")
            # å®‰å…¨å›é€€
            call_amount = game_state.get('call_amount', 0)
            if call_amount == 0:
                return PlayerAction.CHECK, 0
            elif self.chips >= call_amount:
                return PlayerAction.CALL, call_amount
            else:
                return PlayerAction.FOLD, 0
    
    def _should_explore(self, state_key: str) -> bool:
        """æ˜¯å¦åº”è¯¥æ¢ç´¢"""
        return random.random() < self.epsilon
    
    def _get_best_action(self, state_key: str, available_actions: List[Tuple[PlayerAction, int]], 
                        game_state: Dict[str, Any]) -> Tuple[PlayerAction, int]:
        """è·å–æœ€ä½³åŠ¨ä½œ"""
        if self.use_double_q_learning:
            return self._double_q_action_selection(state_key, available_actions, game_state)
        else:
            return self._single_q_action_selection(state_key, available_actions, game_state)
    
    def _single_q_action_selection(self, state_key: str, available_actions: List[Tuple[PlayerAction, int]], 
                                  game_state: Dict[str, Any]) -> Tuple[PlayerAction, int]:
        """å•Qè¡¨åŠ¨ä½œé€‰æ‹©"""
        best_q_value = float('-inf')
        best_action = available_actions[0]
        
        for action, amount in available_actions:
            action_key = self._get_action_key(action, amount, game_state)
            q_value = self.q_table[state_key][action_key]
            
            if q_value > best_q_value:
                best_q_value = q_value
                best_action = (action, amount)
        
        return best_action
    
    def _double_q_action_selection(self, state_key: str, available_actions: List[Tuple[PlayerAction, int]], 
                                  game_state: Dict[str, Any]) -> Tuple[PlayerAction, int]:
        """åŒQè¡¨åŠ¨ä½œé€‰æ‹©"""
        best_q_value = float('-inf')
        best_action = available_actions[0]
        
        for action, amount in available_actions:
            action_key = self._get_action_key(action, amount, game_state)
            # å–ä¸¤ä¸ªQè¡¨çš„å¹³å‡å€¼
            q_value = (self.q_table_1[state_key][action_key] + 
                      self.q_table_2[state_key][action_key]) / 2
            
            if q_value > best_q_value:
                best_q_value = q_value
                best_action = (action, amount)
        
        return best_action
    
    def _get_basic_actions(self, game_state: Dict[str, Any]) -> List[Tuple[PlayerAction, int]]:
        """è·å–åŸºç¡€åŠ¨ä½œé›†åˆ"""
        actions = []
        call_amount = game_state.get('call_amount', 0)
        min_raise = game_state.get('min_raise', game_state.get('big_blind', 20))
        
        # å¼ƒç‰Œ (å¦‚æœéœ€è¦è·Ÿæ³¨)
        if call_amount > 0:
            actions.append((PlayerAction.FOLD, 0))
        
        # è¿‡ç‰Œæˆ–è·Ÿæ³¨
        if call_amount == 0:
            actions.append((PlayerAction.CHECK, 0))
        elif call_amount <= self.chips:
            actions.append((PlayerAction.CALL, call_amount))
        
        # åŠ æ³¨
        if self.chips > call_amount:
            remaining_chips = self.chips - call_amount
            if remaining_chips >= min_raise:
                # å°åŠ æ³¨
                small_raise = min(min_raise, remaining_chips)
                actions.append((PlayerAction.RAISE, call_amount + small_raise))
                
                # å¤§åŠ æ³¨
                if remaining_chips >= min_raise * 3:
                    big_raise = min(min_raise * 3, remaining_chips)
                    actions.append((PlayerAction.RAISE, call_amount + big_raise))
        
        # å…¨æŠ¼
        if self.chips > call_amount:
            actions.append((PlayerAction.ALL_IN, self.chips))
        
        # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªåŠ¨ä½œ
        if not actions:
            if call_amount == 0:
                actions.append((PlayerAction.CHECK, 0))
            elif self.chips >= call_amount:
                actions.append((PlayerAction.CALL, call_amount))
            else:
                actions.append((PlayerAction.FOLD, 0))
        
        return actions
    
    @abstractmethod
    def _get_enhanced_actions(self, game_state: Dict[str, Any]) -> List[Tuple[PlayerAction, int]]:
        """è·å–å¢å¼ºåŠ¨ä½œé›†åˆï¼Œå­ç±»éœ€è¦å®ç°"""
        pass
    
    def _get_action_key(self, action_type: PlayerAction, amount: int, game_state: Dict[str, Any]) -> str:
        """è·å–åŠ¨ä½œé”®å€¼"""
        if action_type in [PlayerAction.FOLD, PlayerAction.CHECK]:
            return action_type.value
        elif action_type == PlayerAction.CALL:
            return f"call_{self._discretize_amount(amount, game_state)}"
        elif action_type == PlayerAction.RAISE:
            return f"raise_{self._discretize_amount(amount, game_state)}"
        elif action_type == PlayerAction.ALL_IN:
            return "all_in"
        else:
            return str(action_type.value)
    
    def _discretize_amount(self, amount: int, game_state: Dict[str, Any]) -> str:
        """ç¦»æ•£åŒ–é‡‘é¢"""
        pot_size = max(1, game_state.get('pot', 1))
        ratio = amount / pot_size
        
        if ratio <= 0.5:
            return "small"
        elif ratio <= 1.5:
            return "medium"
        elif ratio <= 3.0:
            return "large"
        else:
            return "huge"
    
    def learn_from_hand_result(self, hand_result: Dict[str, Any]):
        """ä»æ‰‹ç‰Œç»“æœå­¦ä¹ """
        if not self.current_trajectory:
            return
        
        # è®¡ç®—å¥–åŠ±
        reward = self._calculate_reward(hand_result)
        self.total_reward += reward
        self.game_count += 1
        
        if hand_result.get('winner_id') == self.player_id:
            self.win_count += 1
        
        # æ›´æ–°Qå€¼
        self._update_q_values(reward)
        
        # ç»éªŒå›æ”¾
        if self.use_experience_replay:
            self._add_experience_and_learn(reward)
        
        # è¡°å‡æ¢ç´¢ç‡
        self.decay_epsilon()
        
        # é‡ç½®è½¨è¿¹
        self.current_trajectory = []
        self.last_state_key = None
        self.last_action_key = None
        
        # å®šæœŸè®°å½•è®­ç»ƒè¿›åº¦
        if self.game_count % self.snapshot_interval == 0:
            self._record_training_progress()
    
    @abstractmethod
    def _calculate_reward(self, hand_result: Dict[str, Any]) -> float:
        """è®¡ç®—å¥–åŠ±ï¼Œå­ç±»éœ€è¦å®ç°"""
        pass
    
    def _update_q_values(self, final_reward: float):
        """æ›´æ–°Qå€¼"""
        if self.use_double_q_learning:
            self._update_double_q_values(final_reward)
        else:
            self._update_single_q_values(final_reward)
    
    def _update_single_q_values(self, final_reward: float):
        """æ›´æ–°å•Qè¡¨"""
        reward = final_reward
        for i in reversed(range(len(self.current_trajectory))):
            step = self.current_trajectory[i]
            state = step['state']
            action_key = self._get_action_key(step['action'][0], step['action'][1], step['game_state'])
            
            # è·å–ä¸‹ä¸€çŠ¶æ€çš„æœ€å¤§Qå€¼
            next_max_q = 0
            if i < len(self.current_trajectory) - 1:
                next_step = self.current_trajectory[i + 1]
                next_state = next_step['state']
                next_max_q = max(self.q_table[next_state].values()) if self.q_table[next_state] else 0
            
            # æ›´æ–°Qå€¼
            old_q = self.q_table[state][action_key]
            target_q = reward + self.gamma * next_max_q
            self.q_table[state][action_key] = old_q + self.learning_rate * (target_q - old_q)
            
            # æŠ˜æ‰£å¥–åŠ±
            reward *= self.gamma
    
    def _update_double_q_values(self, final_reward: float):
        """æ›´æ–°åŒQè¡¨"""
        reward = final_reward
        for i in reversed(range(len(self.current_trajectory))):
            step = self.current_trajectory[i]
            state = step['state']
            action_key = self._get_action_key(step['action'][0], step['action'][1], step['game_state'])
            
            # éšæœºé€‰æ‹©æ›´æ–°å“ªä¸ªQè¡¨
            if random.random() < 0.5:
                primary_q, secondary_q = self.q_table_1, self.q_table_2
            else:
                primary_q, secondary_q = self.q_table_2, self.q_table_1
            
            # è·å–ä¸‹ä¸€çŠ¶æ€
            next_state = None
            if i < len(self.current_trajectory) - 1:
                next_step = self.current_trajectory[i + 1]
                next_state = next_step['state']
            
            # æ›´æ–°Qå€¼
            self._update_q_table(primary_q, secondary_q, state, action_key, reward, next_state)
            
            # æŠ˜æ‰£å¥–åŠ±
            reward *= self.gamma
    
    def _update_q_table(self, primary_q: dict, secondary_q: dict, state: str, action: str, 
                       reward: float, next_state: str):
        """æ›´æ–°Qè¡¨"""
        if next_state:
            # ä½¿ç”¨primary_qé€‰æ‹©åŠ¨ä½œï¼Œsecondary_qè¯„ä¼°ä»·å€¼
            best_next_action = max(primary_q[next_state], key=primary_q[next_state].get) if primary_q[next_state] else None
            next_q_value = secondary_q[next_state][best_next_action] if best_next_action else 0
        else:
            next_q_value = 0
        
        # TDæ›´æ–°
        old_q = primary_q[state][action]
        target_q = reward + self.gamma * next_q_value
        primary_q[state][action] = old_q + self.learning_rate * (target_q - old_q)
    
    def _add_experience_and_learn(self, final_reward: float):
        """æ·»åŠ ç»éªŒå¹¶è¿›è¡Œç»éªŒå›æ”¾å­¦ä¹ """
        # æ·»åŠ ç»éªŒåˆ°ç¼“å†²åŒº
        for step in self.current_trajectory:
            experience = {
                'state': step['state'],
                'action': step['action'],
                'reward': final_reward,
                'game_state': step['game_state']
            }
            self.experience_memory.append(experience)
        
        # ç»éªŒå›æ”¾
        if len(self.experience_memory) >= self.replay_batch_size:
            self._replay_experience()
    
    def _replay_experience(self):
        """ç»éªŒå›æ”¾"""
        batch_size = min(self.replay_batch_size, len(self.experience_memory))
        batch = random.sample(list(self.experience_memory), batch_size)
        
        for experience in batch:
            state = experience['state']
            action = experience['action']
            reward = experience['reward']
            game_state = experience['game_state']
            
            action_key = self._get_action_key(action[0], action[1], game_state)
            
            if self.use_double_q_learning:
                # éšæœºé€‰æ‹©Qè¡¨
                if random.random() < 0.5:
                    primary_q, secondary_q = self.q_table_1, self.q_table_2
                else:
                    primary_q, secondary_q = self.q_table_2, self.q_table_1
                
                old_q = primary_q[state][action_key]
                target_q = reward
                primary_q[state][action_key] = old_q + self.learning_rate * (target_q - old_q)
            else:
                old_q = self.q_table[state][action_key]
                target_q = reward
                self.q_table[state][action_key] = old_q + self.learning_rate * (target_q - old_q)
    
    def decay_epsilon(self):
        """è¡°å‡æ¢ç´¢ç‡"""
        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)
    
    def estimate_hand_strength(self, game_state: Dict[str, Any]) -> float:
        """ä¼°ç®—æ‰‹ç‰Œå¼ºåº¦ (0-1)"""
        try:
            if not self.hole_cards or len(self.hole_cards) != 2:
                return 0.3
            
            community_cards = game_state.get('community_cards', [])
            
            if not community_cards:
                return self._evaluate_preflop_strength()
            else:
                all_cards = self.hole_cards + community_cards
                return self._evaluate_partial_hand_strength(all_cards)
        except:
            return 0.3
    
    def _evaluate_preflop_strength(self) -> float:
        """è¯„ä¼°ç¿»ç‰Œå‰æ‰‹ç‰Œå¼ºåº¦"""
        if len(self.hole_cards) != 2:
            return 0.3
        
        card1, card2 = self.hole_cards
        rank1, suit1 = card1.rank, card1.suit
        rank2, suit2 = card2.rank, card2.suit
        
        # å®šä¹‰ç‰Œé¢å€¼
        rank_values = {'2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, 
                      '9': 9, 'T': 10, 'J': 11, 'Q': 12, 'K': 13, 'A': 14}
        
        val1 = rank_values.get(rank1, 7)
        val2 = rank_values.get(rank2, 7)
        
        # åŸºç¡€åˆ†æ•°
        high_card = max(val1, val2)
        low_card = min(val1, val2)
        
        # å¯¹å­
        if val1 == val2:
            if val1 >= 10:  # TT+
                return 0.9
            elif val1 >= 7:  # 77-99
                return 0.75
            else:  # 22-66
                return 0.6
        
        # åŒèŠ±
        suited = (suit1 == suit2)
        
        # è¿ç‰Œ
        connected = abs(val1 - val2) == 1 or (val1 == 14 and val2 == 2)
        
        # AK, AQç±»ä¼¼çš„é«˜ç‰Œ
        if high_card == 14:  # A
            if low_card >= 10:  # AK, AQ, AJ, AT
                return 0.85 if suited else 0.75
            elif low_card >= 7:  # A9-A7
                return 0.65 if suited else 0.5
            else:  # A6-A2
                return 0.55 if suited else 0.4
        
        # é«˜ç‰Œç»„åˆ
        if high_card >= 12:  # K+
            if low_card >= 10:  # KQ, KJ, QJ
                return 0.7 if suited else 0.6
            elif low_card >= 8:  # K9-K8, Q9-Q8
                return 0.55 if suited else 0.45
        
        # åŒèŠ±è¿ç‰Œå¥–åŠ±
        if suited and connected:
            if high_card >= 8:
                return 0.65
            else:
                return 0.55
        
        # è¿ç‰Œ
        if connected and high_card >= 8:
            return 0.5
        
        # åŒèŠ±
        if suited:
            return 0.45
        
        # é»˜è®¤
        return 0.3
    
    def _evaluate_partial_hand_strength(self, all_cards) -> float:
        """è¯„ä¼°éƒ¨åˆ†å…¬å…±ç‰Œæƒ…å†µä¸‹çš„æ‰‹ç‰Œå¼ºåº¦"""
        # ç®€åŒ–å®ç°
        return min(0.9, 0.3 + len(all_cards) * 0.1)
    
    def calculate_pot_odds(self, game_state: Dict[str, Any]) -> float:
        """è®¡ç®—åº•æ± èµ”ç‡"""
        call_amount = game_state.get('call_amount', 0)
        pot_size = game_state.get('pot', 0)
        
        if call_amount <= 0 or pot_size <= 0:
            return float('inf')
        
        return pot_size / call_amount
    
    def save_model(self):
        """ä¿å­˜æ¨¡å‹"""
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(self.model_path), exist_ok=True)
            
            data = {
                'total_reward': self.total_reward,
                'game_count': self.game_count,
                'win_count': self.win_count,
                'epsilon': self.epsilon,
                'state_visit_count': dict(self.state_visit_count),
                'action_count': {k: dict(v) for k, v in self.action_count.items()},
                'config': self.config.__dict__
            }
            
            # ä¿å­˜Qè¡¨
            if self.use_double_q_learning:
                data['q_table_1'] = dict(self.q_table_1)
                data['q_table_2'] = dict(self.q_table_2)
            else:
                data['q_table'] = dict(self.q_table)
            
            with open(self.model_path, 'wb') as f:
                pickle.dump(data, f)
            print(f"ğŸ’¾ {self.config.model_name}æ¨¡å‹å·²ä¿å­˜: {self.model_path}")
        except Exception as e:
            print(f"âŒ ä¿å­˜æ¨¡å‹å¤±è´¥: {e}")
    
    def load_model(self):
        """åŠ è½½æ¨¡å‹"""
        try:
            if os.path.exists(self.model_path):
                with open(self.model_path, 'rb') as f:
                    data = pickle.load(f)
                
                self.total_reward = data.get('total_reward', 0.0)
                self.game_count = data.get('game_count', 0)
                self.win_count = data.get('win_count', 0)
                self.epsilon = data.get('epsilon', self.config.epsilon)
                self.state_visit_count = defaultdict(int, data.get('state_visit_count', {}))
                
                action_count_data = data.get('action_count', {})
                self.action_count = defaultdict(lambda: defaultdict(int))
                for state, actions in action_count_data.items():
                    for action, count in actions.items():
                        self.action_count[state][action] = count
                
                # åŠ è½½Qè¡¨
                if self.use_double_q_learning:
                    q1_data = data.get('q_table_1', {})
                    q2_data = data.get('q_table_2', {})
                    self.q_table_1 = defaultdict(lambda: defaultdict(float))
                    self.q_table_2 = defaultdict(lambda: defaultdict(float))
                    
                    for state, actions in q1_data.items():
                        for action, value in actions.items():
                            self.q_table_1[state][action] = value
                    
                    for state, actions in q2_data.items():
                        for action, value in actions.items():
                            self.q_table_2[state][action] = value
                else:
                    q_data = data.get('q_table', {})
                    self.q_table = defaultdict(lambda: defaultdict(float))
                    for state, actions in q_data.items():
                        for action, value in actions.items():
                            self.q_table[state][action] = value
                
                print(f"âœ… å·²åŠ è½½{self.config.model_name}: {self.model_path}")
                print(f"   æ¸¸æˆæ¬¡æ•°: {self.game_count}, èƒœç‡: {(self.win_count/max(1,self.game_count))*100:.1f}%")
            else:
                self._initialize_empty_model()
        except Exception as e:
            print(f"âš ï¸  åŠ è½½æ¨¡å‹å¤±è´¥ ({e})ï¼Œåˆ›å»ºæ–°æ¨¡å‹")
            self._initialize_empty_model()
    
    def _initialize_empty_model(self):
        """åˆå§‹åŒ–ç©ºæ¨¡å‹"""
        print(f"ğŸ“ åˆ›å»ºæ–°çš„{self.config.model_name}: {self.model_path}")
    
    def get_learning_stats(self) -> Dict[str, Any]:
        """è·å–å­¦ä¹ ç»Ÿè®¡ä¿¡æ¯"""
        if self.use_double_q_learning:
            q_table_size = len(self.q_table_1) + len(self.q_table_2)
        else:
            q_table_size = len(getattr(self, 'q_table', {}))
        
        stats = {
            'q_table_size': q_table_size,
            'epsilon': self.epsilon,
            'total_reward': self.total_reward,
            'game_count': self.game_count,
            'win_count': self.win_count,
            'win_rate': (self.win_count / max(1, self.game_count)) * 100,
            'avg_reward': self.total_reward / max(1, self.game_count),
            'memory_size': len(getattr(self, 'experience_memory', [])),
            'total_states': len(self.state_visit_count),
            'model_name': self.config.model_name
        }
        
        # æ·»åŠ é…ç½®ä¿¡æ¯
        stats.update(self.config.__dict__)
        
        return stats
    
    def _record_training_progress(self):
        """è®°å½•è®­ç»ƒè¿›åº¦"""
        try:
            from .training_tracker import TrainingTracker
            
            tracker = TrainingTracker()
            current_stats = self.get_learning_stats()
            
            additional_info = {
                'snapshot_interval': self.snapshot_interval,
                'model_path': self.model_path,
                'current_chips': self.chips
            }
            
            tracker.record_snapshot(self.config.model_name, current_stats, additional_info)
        except Exception as e:
            # é™é»˜å¤„ç†é”™è¯¯ï¼Œä¸å½±å“è®­ç»ƒ
            pass 