# 🤖 强化学习机器人统计逻辑详细分析

## 📊 统计指标概览

强化学习机器人主要统计以下核心指标：

- **reward（奖励）**：每手牌结束后的学习信号
- **胜率（win_rate）**：获胜手牌数占总手牌数的比例
- **total_reward**：累计总奖励
- **avg_reward**：平均每手牌奖励
- **game_count**：游戏局数
- **win_count**：获胜局数

## 🔄 统计流程

### 1. 手牌进行中

```python
# 在每次决策时记录轨迹
self.current_trajectory.append({
    'state': state_key,
    'action': (action, amount), 
    'game_state': game_state.copy()
})

# 记录总下注金额
self.total_bet_in_hand += bet_amount
```

### 2. 手牌结束时触发学习

```python
def learn_from_hand_result(self, hand_result: Dict[str, Any]):
    """从手牌结果学习 - 核心统计逻辑"""
    if not self.current_trajectory:
        return
  
    # 🎯 计算奖励
    reward = self._calculate_reward(hand_result)
    self.total_reward += reward
    self.game_count += 1
  
    # 📈 更新胜率统计
    if hand_result.get('winner_id') == self.player_id:
        self.win_count += 1
  
    # 🧠 更新Q值（强化学习核心）
    self._update_q_values(reward)
  
    # 💾 经验回放（如果启用）
    if self.use_experience_replay:
        self._add_experience_and_learn(reward)
  
    # 🎲 衰减探索率
    self.decay_epsilon()
  
    # 🔄 重置轨迹
    self.current_trajectory = []
    self.total_bet_in_hand = 0
    self.is_folded = False
```

### 3. hand_result数据结构

```python
hand_result = {
    'winner_id': str,        # 获胜者ID
    'winnings': int,         # 该玩家获得的奖金
    'game_state': dict       # 游戏状态信息
}
```

## 💰 Reward计算逻辑

### 原版机器人（RLBot）- 简单奖励

```python
def _calculate_reward(self, hand_result: Dict[str, Any]) -> float:
    """简单的胜负奖励"""
    if hand_result.get('winner_id') == self.player_id:
        # 🏆 获胜奖励 = 奖金 / 投入
        winnings = hand_result.get('winnings', 0)
        roi_reward = winnings / max(self.total_bet_in_hand, 1)
        reward = min(10.0, roi_reward)  # 限制最大奖励
    else:
        # 💸 失败惩罚 = -投入/总筹码
        reward = -self.total_bet_in_hand / max(self.chips + self.total_bet_in_hand, 1)
  
    # 😴 弃牌小惩罚
    if self.is_folded:
        reward = -0.1
  
    return reward
```

### 改进版机器人（ImprovedRLBot）- 复杂奖励

```python
def _calculate_reward(self, hand_result: Dict[str, Any]) -> float:
    """增强奖励函数，考虑决策质量"""
    reward = 0.0
  
    # 1️⃣ 基础胜负奖励
    if hand_result.get('winner_id') == self.player_id:
        winnings = hand_result.get('winnings', 0)
        roi_reward = winnings / max(self.total_bet_in_hand, 1)
        reward += min(5.0, roi_reward)
    else:
        loss_ratio = self.total_bet_in_hand / max(self.chips + self.total_bet_in_hand, 1)
        reward -= min(1.5, loss_ratio * 1.0)
  
    # 2️⃣ 决策质量奖励
    for step in self.current_trajectory:
        action, amount = step['action']
        hand_strength = self.estimate_hand_strength(step['game_state'])
        pot_odds = self.calculate_pot_odds(step['game_state'])
      
        # 奖励正确的激进决策
        if action == PlayerAction.RAISE and hand_strength > 0.6:
            reward += 0.1  # 强牌加注
        elif action == PlayerAction.FOLD and hand_strength < 0.2:
            reward += 0.05  # 弱牌弃牌
        elif action == PlayerAction.CALL and pot_odds > 2.0:
            reward += 0.05  # 根据赔率跟注
  
    # 3️⃣ 生存奖励
    if not self.is_folded and self.chips > 0:
        reward += 0.05
  
    # 4️⃣ 适应性奖励
    if hasattr(self, 'recent_performance'):
        if len(self.recent_performance) >= 5:
            recent_avg = sum(self.recent_performance[-5:]) / 5
            if recent_avg > 0:
                reward += 0.1
  
    return reward
```

### 保守版机器人（ConservativeRLBot）- 保守奖励

```python
def _calculate_reward(self, hand_result: Dict[str, Any]) -> float:
    """保守策略的奖励函数"""
    reward = 0.0
  
    # 1️⃣ 基础胜负奖励（同改进版）
    # ...
  
    # 2️⃣ 保守决策质量奖励
    for step in self.current_trajectory:
        action = step['action'][0]
        hand_strength = self.estimate_hand_strength(step['game_state'])
      
        # 更高的保守决策奖励
        if action == PlayerAction.FOLD and hand_strength < 0.25:
            reward += 0.08  # 比其他机器人更高
        elif action == PlayerAction.CHECK and hand_strength < 0.4:
            reward += 0.05
        elif action == PlayerAction.RAISE and hand_strength >= 0.7:
            reward += 0.12  # 鼓励价值下注
      
        # 惩罚过度激进
        if action == PlayerAction.RAISE and hand_strength < 0.4:
            reward -= 0.1
        elif action == PlayerAction.ALL_IN and hand_strength < 0.8:
            reward -= 0.15
  
    # 3️⃣ 更高的生存奖励
    if not self.is_folded and self.chips > 0:
        reward += 0.1  # 比其他机器人更高
  
    # 4️⃣ 筹码管理奖励
    if hasattr(self, 'initial_chips'):
        chips_ratio = self.chips / max(self.initial_chips, 1)
        if chips_ratio > 1.1:
            reward += 0.05  # 筹码增长
        elif chips_ratio < 0.5:
            reward -= 0.1   # 筹码损失过多
  
    # 5️⃣ 一致性奖励（70%以上保守动作）
    conservative_actions = [PlayerAction.FOLD, PlayerAction.CHECK, PlayerAction.CALL]
    conservative_count = sum(1 for step in self.current_trajectory 
                           if step['action'][0] in conservative_actions)
    if len(self.current_trajectory) > 0:
        conservative_ratio = conservative_count / len(self.current_trajectory)
        if conservative_ratio >= 0.7:
            reward += 0.05
  
    return reward
```

## 📈 胜率统计逻辑

### 胜负判定

```python
# 在learn_from_hand_result中
if hand_result.get('winner_id') == self.player_id:
    self.win_count += 1  # 获胜计数+1

self.game_count += 1     # 总局数+1

# 胜率计算
win_rate = (self.win_count / max(1, self.game_count)) * 100
```

### 获胜条件

在游戏引擎中，获胜者通过以下逻辑确定：

```python
def _get_hand_winner_id(self) -> str:
    """获取手牌获胜者ID"""
    active_players = [p for p in self.players if not p.is_folded]
    if len(active_players) == 1:
        return active_players[0].player_id
    # 如果多人到摊牌，比较手牌大小（实际实现更复杂）
```

### 奖金计算

```python
def _get_player_winnings(self, player: Player) -> int:
    """获取玩家奖金"""
    if not player.is_folded:
        active_players = [p for p in self.players if not p.is_folded]
        if len(active_players) == 1:
            return self.pot  # 独赢底池
    return 0  # 未获胜
```

## 🎯 关键统计变量

### 实例变量

```python
class BaseRLBot:
    def __init__(self):
        # 核心统计
        self.total_reward = 0.0      # 累计总奖励
        self.game_count = 0          # 游戏局数
        self.win_count = 0           # 获胜局数
      
        # 当前手牌状态
        self.current_trajectory = [] # 当前手牌的决策轨迹
        self.total_bet_in_hand = 0   # 当前手牌总投入
        self.is_folded = False       # 是否已弃牌
      
        # 学习参数
        self.epsilon = config.epsilon # 探索率
      
        # Q学习核心
        self.q_table = {}            # Q值表（或双Q表）
```

### 计算属性

```python
def get_learning_stats(self) -> Dict[str, Any]:
    """获取学习统计"""
    return {
        'total_reward': self.total_reward,
        'game_count': self.game_count,
        'win_count': self.win_count,
        'win_rate': (self.win_count / max(1, self.game_count)) * 100,
        'avg_reward': self.total_reward / max(1, self.game_count),
        # ...其他统计
    }
```

## 📊 统计数据的持久化

### 保存到模型文件

```python
def save_model(self):
    """保存模型和统计数据"""
    data = {
        'total_reward': self.total_reward,
        'game_count': self.game_count,
        'win_count': self.win_count,
        'epsilon': self.epsilon,
        'q_table': dict(self.q_table),
        'state_visit_count': dict(self.state_visit_count),
        'action_count': {k: dict(v) for k, v in self.action_count.items()},
        'config': self.config.__dict__
    }
  
    with open(self.model_path, 'wb') as f:
        pickle.dump(data, f)
```

### 从模型文件加载

```python
def load_model(self):
    """加载模型和统计数据"""
    with open(self.model_path, 'rb') as f:
        data = pickle.load(f)
  
    self.total_reward = data.get('total_reward', 0.0)
    self.game_count = data.get('game_count', 0)
    self.win_count = data.get('win_count', 0)
    self.epsilon = data.get('epsilon', self.config.epsilon)
    # ...加载其他数据
```

## 🔄 训练过程中的统计更新

### 定期快照

```python
def _record_training_progress(self):
    """记录训练进度（每50局调用一次）"""
    if self.game_count % self.snapshot_interval == 0:
        from .training_tracker import TrainingTracker
      
        tracker = TrainingTracker()
        current_stats = self.get_learning_stats()
        tracker.record_snapshot(self.config.model_name, current_stats)
```

### 实时更新

每手牌结束时自动更新所有统计指标，确保数据准确性。

## 💡 统计逻辑总结

1. **胜率统计**：基于手牌级别的胜负，每手牌结束后更新
2. **Reward计算**：考虑多个因素（ROI、决策质量、生存能力等）
3. **数据持久化**：保存到pkl文件，支持训练中断恢复
4. **实时追踪**：每局自动更新，定期生成快照
5. **差异化奖励**：不同机器人类型使用不同的奖励函数

这套统计系统既保证了强化学习的有效性，又提供了丰富的性能分析数据。
