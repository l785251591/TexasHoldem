# ğŸ¤– å¼ºåŒ–å­¦ä¹ æœºå™¨äººç»Ÿè®¡é€»è¾‘è¯¦ç»†åˆ†æ

## ğŸ“Š ç»Ÿè®¡æŒ‡æ ‡æ¦‚è§ˆ

å¼ºåŒ–å­¦ä¹ æœºå™¨äººä¸»è¦ç»Ÿè®¡ä»¥ä¸‹æ ¸å¿ƒæŒ‡æ ‡ï¼š

- **rewardï¼ˆå¥–åŠ±ï¼‰**ï¼šæ¯æ‰‹ç‰Œç»“æŸåçš„å­¦ä¹ ä¿¡å·
- **èƒœç‡ï¼ˆwin_rateï¼‰**ï¼šè·èƒœæ‰‹ç‰Œæ•°å æ€»æ‰‹ç‰Œæ•°çš„æ¯”ä¾‹
- **total_reward**ï¼šç´¯è®¡æ€»å¥–åŠ±
- **avg_reward**ï¼šå¹³å‡æ¯æ‰‹ç‰Œå¥–åŠ±
- **game_count**ï¼šæ¸¸æˆå±€æ•°
- **win_count**ï¼šè·èƒœå±€æ•°

## ğŸ”„ ç»Ÿè®¡æµç¨‹

### 1. æ‰‹ç‰Œè¿›è¡Œä¸­

```python
# åœ¨æ¯æ¬¡å†³ç­–æ—¶è®°å½•è½¨è¿¹
self.current_trajectory.append({
    'state': state_key,
    'action': (action, amount), 
    'game_state': game_state.copy()
})

# è®°å½•æ€»ä¸‹æ³¨é‡‘é¢
self.total_bet_in_hand += bet_amount
```

### 2. æ‰‹ç‰Œç»“æŸæ—¶è§¦å‘å­¦ä¹ 

```python
def learn_from_hand_result(self, hand_result: Dict[str, Any]):
    """ä»æ‰‹ç‰Œç»“æœå­¦ä¹  - æ ¸å¿ƒç»Ÿè®¡é€»è¾‘"""
    if not self.current_trajectory:
        return
  
    # ğŸ¯ è®¡ç®—å¥–åŠ±
    reward = self._calculate_reward(hand_result)
    self.total_reward += reward
    self.game_count += 1
  
    # ğŸ“ˆ æ›´æ–°èƒœç‡ç»Ÿè®¡
    if hand_result.get('winner_id') == self.player_id:
        self.win_count += 1
  
    # ğŸ§  æ›´æ–°Qå€¼ï¼ˆå¼ºåŒ–å­¦ä¹ æ ¸å¿ƒï¼‰
    self._update_q_values(reward)
  
    # ğŸ’¾ ç»éªŒå›æ”¾ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if self.use_experience_replay:
        self._add_experience_and_learn(reward)
  
    # ğŸ² è¡°å‡æ¢ç´¢ç‡
    self.decay_epsilon()
  
    # ğŸ”„ é‡ç½®è½¨è¿¹
    self.current_trajectory = []
    self.total_bet_in_hand = 0
    self.is_folded = False
```

### 3. hand_resultæ•°æ®ç»“æ„

```python
hand_result = {
    'winner_id': str,        # è·èƒœè€…ID
    'winnings': int,         # è¯¥ç©å®¶è·å¾—çš„å¥–é‡‘
    'game_state': dict       # æ¸¸æˆçŠ¶æ€ä¿¡æ¯
}
```

## ğŸ’° Rewardè®¡ç®—é€»è¾‘

### åŸç‰ˆæœºå™¨äººï¼ˆRLBotï¼‰- ç®€å•å¥–åŠ±

```python
def _calculate_reward(self, hand_result: Dict[str, Any]) -> float:
    """ç®€å•çš„èƒœè´Ÿå¥–åŠ±"""
    if hand_result.get('winner_id') == self.player_id:
        # ğŸ† è·èƒœå¥–åŠ± = å¥–é‡‘ / æŠ•å…¥
        winnings = hand_result.get('winnings', 0)
        roi_reward = winnings / max(self.total_bet_in_hand, 1)
        reward = min(10.0, roi_reward)  # é™åˆ¶æœ€å¤§å¥–åŠ±
    else:
        # ğŸ’¸ å¤±è´¥æƒ©ç½š = -æŠ•å…¥/æ€»ç­¹ç 
        reward = -self.total_bet_in_hand / max(self.chips + self.total_bet_in_hand, 1)
  
    # ğŸ˜´ å¼ƒç‰Œå°æƒ©ç½š
    if self.is_folded:
        reward = -0.1
  
    return reward
```

### æ”¹è¿›ç‰ˆæœºå™¨äººï¼ˆImprovedRLBotï¼‰- å¤æ‚å¥–åŠ±

```python
def _calculate_reward(self, hand_result: Dict[str, Any]) -> float:
    """å¢å¼ºå¥–åŠ±å‡½æ•°ï¼Œè€ƒè™‘å†³ç­–è´¨é‡"""
    reward = 0.0
  
    # 1ï¸âƒ£ åŸºç¡€èƒœè´Ÿå¥–åŠ±
    if hand_result.get('winner_id') == self.player_id:
        winnings = hand_result.get('winnings', 0)
        roi_reward = winnings / max(self.total_bet_in_hand, 1)
        reward += min(5.0, roi_reward)
    else:
        loss_ratio = self.total_bet_in_hand / max(self.chips + self.total_bet_in_hand, 1)
        reward -= min(1.5, loss_ratio * 1.0)
  
    # 2ï¸âƒ£ å†³ç­–è´¨é‡å¥–åŠ±
    for step in self.current_trajectory:
        action, amount = step['action']
        hand_strength = self.estimate_hand_strength(step['game_state'])
        pot_odds = self.calculate_pot_odds(step['game_state'])
      
        # å¥–åŠ±æ­£ç¡®çš„æ¿€è¿›å†³ç­–
        if action == PlayerAction.RAISE and hand_strength > 0.6:
            reward += 0.1  # å¼ºç‰ŒåŠ æ³¨
        elif action == PlayerAction.FOLD and hand_strength < 0.2:
            reward += 0.05  # å¼±ç‰Œå¼ƒç‰Œ
        elif action == PlayerAction.CALL and pot_odds > 2.0:
            reward += 0.05  # æ ¹æ®èµ”ç‡è·Ÿæ³¨
  
    # 3ï¸âƒ£ ç”Ÿå­˜å¥–åŠ±
    if not self.is_folded and self.chips > 0:
        reward += 0.05
  
    # 4ï¸âƒ£ é€‚åº”æ€§å¥–åŠ±
    if hasattr(self, 'recent_performance'):
        if len(self.recent_performance) >= 5:
            recent_avg = sum(self.recent_performance[-5:]) / 5
            if recent_avg > 0:
                reward += 0.1
  
    return reward
```

### ä¿å®ˆç‰ˆæœºå™¨äººï¼ˆConservativeRLBotï¼‰- ä¿å®ˆå¥–åŠ±

```python
def _calculate_reward(self, hand_result: Dict[str, Any]) -> float:
    """ä¿å®ˆç­–ç•¥çš„å¥–åŠ±å‡½æ•°"""
    reward = 0.0
  
    # 1ï¸âƒ£ åŸºç¡€èƒœè´Ÿå¥–åŠ±ï¼ˆåŒæ”¹è¿›ç‰ˆï¼‰
    # ...
  
    # 2ï¸âƒ£ ä¿å®ˆå†³ç­–è´¨é‡å¥–åŠ±
    for step in self.current_trajectory:
        action = step['action'][0]
        hand_strength = self.estimate_hand_strength(step['game_state'])
      
        # æ›´é«˜çš„ä¿å®ˆå†³ç­–å¥–åŠ±
        if action == PlayerAction.FOLD and hand_strength < 0.25:
            reward += 0.08  # æ¯”å…¶ä»–æœºå™¨äººæ›´é«˜
        elif action == PlayerAction.CHECK and hand_strength < 0.4:
            reward += 0.05
        elif action == PlayerAction.RAISE and hand_strength >= 0.7:
            reward += 0.12  # é¼“åŠ±ä»·å€¼ä¸‹æ³¨
      
        # æƒ©ç½šè¿‡åº¦æ¿€è¿›
        if action == PlayerAction.RAISE and hand_strength < 0.4:
            reward -= 0.1
        elif action == PlayerAction.ALL_IN and hand_strength < 0.8:
            reward -= 0.15
  
    # 3ï¸âƒ£ æ›´é«˜çš„ç”Ÿå­˜å¥–åŠ±
    if not self.is_folded and self.chips > 0:
        reward += 0.1  # æ¯”å…¶ä»–æœºå™¨äººæ›´é«˜
  
    # 4ï¸âƒ£ ç­¹ç ç®¡ç†å¥–åŠ±
    if hasattr(self, 'initial_chips'):
        chips_ratio = self.chips / max(self.initial_chips, 1)
        if chips_ratio > 1.1:
            reward += 0.05  # ç­¹ç å¢é•¿
        elif chips_ratio < 0.5:
            reward -= 0.1   # ç­¹ç æŸå¤±è¿‡å¤š
  
    # 5ï¸âƒ£ ä¸€è‡´æ€§å¥–åŠ±ï¼ˆ70%ä»¥ä¸Šä¿å®ˆåŠ¨ä½œï¼‰
    conservative_actions = [PlayerAction.FOLD, PlayerAction.CHECK, PlayerAction.CALL]
    conservative_count = sum(1 for step in self.current_trajectory 
                           if step['action'][0] in conservative_actions)
    if len(self.current_trajectory) > 0:
        conservative_ratio = conservative_count / len(self.current_trajectory)
        if conservative_ratio >= 0.7:
            reward += 0.05
  
    return reward
```

## ğŸ“ˆ èƒœç‡ç»Ÿè®¡é€»è¾‘

### èƒœè´Ÿåˆ¤å®š

```python
# åœ¨learn_from_hand_resultä¸­
if hand_result.get('winner_id') == self.player_id:
    self.win_count += 1  # è·èƒœè®¡æ•°+1

self.game_count += 1     # æ€»å±€æ•°+1

# èƒœç‡è®¡ç®—
win_rate = (self.win_count / max(1, self.game_count)) * 100
```

### è·èƒœæ¡ä»¶

åœ¨æ¸¸æˆå¼•æ“ä¸­ï¼Œè·èƒœè€…é€šè¿‡ä»¥ä¸‹é€»è¾‘ç¡®å®šï¼š

```python
def _get_hand_winner_id(self) -> str:
    """è·å–æ‰‹ç‰Œè·èƒœè€…ID"""
    active_players = [p for p in self.players if not p.is_folded]
    if len(active_players) == 1:
        return active_players[0].player_id
    # å¦‚æœå¤šäººåˆ°æ‘Šç‰Œï¼Œæ¯”è¾ƒæ‰‹ç‰Œå¤§å°ï¼ˆå®é™…å®ç°æ›´å¤æ‚ï¼‰
```

### å¥–é‡‘è®¡ç®—

```python
def _get_player_winnings(self, player: Player) -> int:
    """è·å–ç©å®¶å¥–é‡‘"""
    if not player.is_folded:
        active_players = [p for p in self.players if not p.is_folded]
        if len(active_players) == 1:
            return self.pot  # ç‹¬èµ¢åº•æ± 
    return 0  # æœªè·èƒœ
```

## ğŸ¯ å…³é”®ç»Ÿè®¡å˜é‡

### å®ä¾‹å˜é‡

```python
class BaseRLBot:
    def __init__(self):
        # æ ¸å¿ƒç»Ÿè®¡
        self.total_reward = 0.0      # ç´¯è®¡æ€»å¥–åŠ±
        self.game_count = 0          # æ¸¸æˆå±€æ•°
        self.win_count = 0           # è·èƒœå±€æ•°
      
        # å½“å‰æ‰‹ç‰ŒçŠ¶æ€
        self.current_trajectory = [] # å½“å‰æ‰‹ç‰Œçš„å†³ç­–è½¨è¿¹
        self.total_bet_in_hand = 0   # å½“å‰æ‰‹ç‰Œæ€»æŠ•å…¥
        self.is_folded = False       # æ˜¯å¦å·²å¼ƒç‰Œ
      
        # å­¦ä¹ å‚æ•°
        self.epsilon = config.epsilon # æ¢ç´¢ç‡
      
        # Qå­¦ä¹ æ ¸å¿ƒ
        self.q_table = {}            # Qå€¼è¡¨ï¼ˆæˆ–åŒQè¡¨ï¼‰
```

### è®¡ç®—å±æ€§

```python
def get_learning_stats(self) -> Dict[str, Any]:
    """è·å–å­¦ä¹ ç»Ÿè®¡"""
    return {
        'total_reward': self.total_reward,
        'game_count': self.game_count,
        'win_count': self.win_count,
        'win_rate': (self.win_count / max(1, self.game_count)) * 100,
        'avg_reward': self.total_reward / max(1, self.game_count),
        # ...å…¶ä»–ç»Ÿè®¡
    }
```

## ğŸ“Š ç»Ÿè®¡æ•°æ®çš„æŒä¹…åŒ–

### ä¿å­˜åˆ°æ¨¡å‹æ–‡ä»¶

```python
def save_model(self):
    """ä¿å­˜æ¨¡å‹å’Œç»Ÿè®¡æ•°æ®"""
    data = {
        'total_reward': self.total_reward,
        'game_count': self.game_count,
        'win_count': self.win_count,
        'epsilon': self.epsilon,
        'q_table': dict(self.q_table),
        'state_visit_count': dict(self.state_visit_count),
        'action_count': {k: dict(v) for k, v in self.action_count.items()},
        'config': self.config.__dict__
    }
  
    with open(self.model_path, 'wb') as f:
        pickle.dump(data, f)
```

### ä»æ¨¡å‹æ–‡ä»¶åŠ è½½

```python
def load_model(self):
    """åŠ è½½æ¨¡å‹å’Œç»Ÿè®¡æ•°æ®"""
    with open(self.model_path, 'rb') as f:
        data = pickle.load(f)
  
    self.total_reward = data.get('total_reward', 0.0)
    self.game_count = data.get('game_count', 0)
    self.win_count = data.get('win_count', 0)
    self.epsilon = data.get('epsilon', self.config.epsilon)
    # ...åŠ è½½å…¶ä»–æ•°æ®
```

## ğŸ”„ è®­ç»ƒè¿‡ç¨‹ä¸­çš„ç»Ÿè®¡æ›´æ–°

### å®šæœŸå¿«ç…§

```python
def _record_training_progress(self):
    """è®°å½•è®­ç»ƒè¿›åº¦ï¼ˆæ¯50å±€è°ƒç”¨ä¸€æ¬¡ï¼‰"""
    if self.game_count % self.snapshot_interval == 0:
        from .training_tracker import TrainingTracker
      
        tracker = TrainingTracker()
        current_stats = self.get_learning_stats()
        tracker.record_snapshot(self.config.model_name, current_stats)
```

### å®æ—¶æ›´æ–°

æ¯æ‰‹ç‰Œç»“æŸæ—¶è‡ªåŠ¨æ›´æ–°æ‰€æœ‰ç»Ÿè®¡æŒ‡æ ‡ï¼Œç¡®ä¿æ•°æ®å‡†ç¡®æ€§ã€‚

## ğŸ’¡ ç»Ÿè®¡é€»è¾‘æ€»ç»“

1. **èƒœç‡ç»Ÿè®¡**ï¼šåŸºäºæ‰‹ç‰Œçº§åˆ«çš„èƒœè´Ÿï¼Œæ¯æ‰‹ç‰Œç»“æŸåæ›´æ–°
2. **Rewardè®¡ç®—**ï¼šè€ƒè™‘å¤šä¸ªå› ç´ ï¼ˆROIã€å†³ç­–è´¨é‡ã€ç”Ÿå­˜èƒ½åŠ›ç­‰ï¼‰
3. **æ•°æ®æŒä¹…åŒ–**ï¼šä¿å­˜åˆ°pklæ–‡ä»¶ï¼Œæ”¯æŒè®­ç»ƒä¸­æ–­æ¢å¤
4. **å®æ—¶è¿½è¸ª**ï¼šæ¯å±€è‡ªåŠ¨æ›´æ–°ï¼Œå®šæœŸç”Ÿæˆå¿«ç…§
5. **å·®å¼‚åŒ–å¥–åŠ±**ï¼šä¸åŒæœºå™¨äººç±»å‹ä½¿ç”¨ä¸åŒçš„å¥–åŠ±å‡½æ•°

è¿™å¥—ç»Ÿè®¡ç³»ç»Ÿæ—¢ä¿è¯äº†å¼ºåŒ–å­¦ä¹ çš„æœ‰æ•ˆæ€§ï¼Œåˆæä¾›äº†ä¸°å¯Œçš„æ€§èƒ½åˆ†ææ•°æ®ã€‚
